import torch
import torchvision.models as models
import torchvision.transforms as transforms

from PIL import Image
from torch.nn.utils.rnn import pack_padded_sequence
from model import EncoderCNN, DecoderRNN
from build_vocab import Vocabulary
from data_loader import get_loader

transform = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))
])

encoder = models.resnet50(pretrained=True)
encoder = EncoderCNN(embed_size)
encoder.eval()  # Evaluation mode

vocab = Vocabulary(vocab_path, vocab_threshold)

decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)
decoder.eval()  # Evaluation mode

encoder.load_state_dict(torch.load(encoder_path, map_location=device))
decoder.load_state_dict(torch.load(decoder_path, map_location=device))

image = Image.open(image_path).convert('RGB')
image = transform(image).unsqueeze(0)  # Add batch dimension

features = encoder(image)
sampled_ids = decoder.sample(features)
sampled_ids = sampled_ids[0].cpu().numpy()

sampled_caption = []
for word_id in sampled_ids:
    word = vocab.idx2word[word_id]
    sampled_caption.append(word)
    if word == '<end>':
        break
sentence = ' '.join(sampled_caption[1:-1])  # Remove <start> and <end> tokens

print("Generated Caption:", sentence)

